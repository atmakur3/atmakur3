# 👋 Hi, I'm Vani Atmakur

## 🚀 Data Engineer | Cloud Data Architect | ETL Specialist

[![Email](https://img.shields.io/badge/Email-vanib.0398%40gmail.com-red)](mailto:vanib.0398@gmail.com)
[![Phone](https://img.shields.io/badge/Phone-%2B1--917--271--8933-blue)](tel:+19172718933)
[![GitHub](https://img.shields.io/badge/GitHub-atmakur3-black?logo=github)](https://github.com/atmakur3)

---

## 💼 Professional Summary

Experienced Data Engineer with over **5 years** of expertise in designing and building complex, scalable data solutions across **AWS, GCP, and Azure**. Specialized in architecting ETL pipelines, data warehousing, and real-time data processing systems that drive business insights and operational excellence.

- 🎯 **Core Expertise**: ETL Pipeline Architecture, Data Warehousing, Cloud Data Engineering
- ☁️ **Cloud Platforms**: AWS (Glue, Redshift, EMR, Lambda, S3), GCP (BigQuery, Dataflow, Composer), Azure (ADF, Databricks, Synapse)
- 💻 **Tech Stack**: Python, PySpark, SQL, Spark, Kafka, Airflow, Hive
- 📊 **Focus Areas**: Performance Optimization, Cost Reduction, Data Governance, Real-time Processing

---

## 🔧 Technical Skills

**Cloud Platforms**  
- AWS: S3, Glue, Redshift, EMR, Lambda, Kinesis, FireHose, IAM  
- GCP: BigQuery, Dataflow, Composer, PubSub, Cloud Functions  
- Azure: ADF, Databricks, Synapse, ADLS

**Programming & Scripting**  
Python | PySpark | SQL | Shell Scripting

**Big Data & ETL Tools**  
Apache Spark | Kafka | Airflow | Hive | Informatica

**Databases & Warehouses**  
Snowflake | BigQuery | Redshift | Azure SQL | Teradata | Oracle

**DevOps & Version Control**  
Git | Jenkins | Azure DevOps | CI/CD Pipelines

**Data Visualization**  
Power BI | Tableau | Looker

---

## 🎯 Featured Projects

### 1. 🏦 AWS ETL Pipeline & Data Warehouse - Capital One (2024-Present)

**Enterprise-Scale Banking Data Infrastructure**

#### Technical Architecture
- **ETL Pipeline**: AWS Glue with PySpark for automated data transformations
- **Data Warehouse**: Amazon Redshift with optimized distribution keys, sort keys, and materialized views
- **Data Lake**: S3-based data lake with partitioned storage, lifecycle policies, and access controls
- **Real-time Processing**: AWS Lambda for event-driven data ingestion from streaming and batch sources
- **Orchestration**: AWS CodePipeline, Jenkins, and GitHub for CI/CD automation

#### Key Technologies
`AWS Glue` `Amazon Redshift` `S3` `Lambda` `EMR` `PySpark` `Python` `SQL` `CloudWatch` `CloudTrail`

#### Business Impact
- ✅ **40% improvement** in query performance through Redshift optimization
- ✅ Reduced manual processing overheads and operational delays
- ✅ Near real-time analytics with minimized latency in decision-making
- ✅ Cost-effective data lake with optimized storage and retention policies
- ✅ Automated CI/CD deployment with error-free releases

#### Key Achievements
- Designed scalable ETL pipelines integrating diverse data sources for analytics
- Built automated workflows reducing manual processing time significantly
- Implemented enterprise-wide data governance, security, and compliance standards
- Optimized large-scale data processing tasks using Amazon EMR with Spark

---

### 2. 🏥 GCP Healthcare Data Pipelines - Humana (2023-2024)

**HIPAA-Compliant Healthcare Analytics Platform**

#### Technical Architecture
- **Streaming Ingestion**: Google Cloud PubSub for real-time data ingestion
- **Data Processing**: Google Dataflow (Apache Beam) for batch and streaming pipelines
- **Data Warehouse**: BigQuery with partitioned and clustered tables
- **Orchestration**: Cloud Composer (Apache Airflow) for workflow management
- **Security**: IAM roles, service accounts, and VPC controls for HIPAA compliance

#### Key Technologies
`BigQuery` `Dataflow` `PubSub` `Cloud Composer` `Airflow` `Apache Beam` `Python` `SQL` `Cloud Functions`

#### Business Impact
- ✅ **60% improvement** in ETL execution time after migration from legacy systems
- ✅ Near real-time analytics for population health monitoring and patient outreach
- ✅ Strict HIPAA compliance with enterprise data governance policies
- ✅ Reduced latency in clinical and operational data feeds
- ✅ Cost-efficient analytics across patient care, provider engagement, and prescription datasets

#### Key Achievements
- Built scalable pipelines processing clinical, claims, and pharmacy data
- Designed healthcare-focused BigQuery warehouses with optimized performance
- Created validation and quality control routines for patient data standardization
- Migrated legacy ETL workloads from Oracle and Teradata to BigQuery
- Partnered with data science teams for predictive modeling (readmission prediction, medication adherence)

---

### 3. 🛍️ Azure Data Engineering Platform - Unilever (2020-2022)

**Global Supply Chain & Sales Analytics Infrastructure**

#### Technical Architecture
- **ETL Pipeline**: Azure Data Factory (ADF) for data integration
- **Data Processing**: Azure Databricks with PySpark and SQL for transformations
- **Data Warehouse**: Azure Synapse Analytics with star and snowflake schemas
- **Real-time Streaming**: Azure Event Hub and Stream Analytics
- **Security**: Azure Key Vault, Managed Identities, RBAC for data protection

#### Key Technologies
`Azure Data Factory` `Azure Databricks` `Azure Synapse` `PySpark` `SQL` `Event Hub` `Stream Analytics` `Azure DevOps`

#### Business Impact
- ✅ Improved query response times for high-volume sales and supply chain datasets
- ✅ Minimal downtime during legacy SQL Server to Azure migration
- ✅ Near real-time dashboards for production, logistics, and inventory insights
- ✅ Automated ADF pipeline deployments with CI/CD using Azure DevOps
- ✅ Enhanced data governance with SCD techniques for historical data management

#### Key Achievements
- Designed end-to-end ETL pipelines integrating diverse data sources into centralized data lakes
- Built and optimized Synapse Analytics data models with partitioning and indexing
- Implemented scalable data processing workflows handling terabytes of data
- Migrated legacy on-premises workloads to Azure with performance optimization
- Created real-time ingestion pipelines enabling timely business insights

---

## 📈 Professional Experience

### 💼 Data Engineer @ Capital One
**Jul 2024 - Present**
- Architecting AWS-native data solutions with Glue, Redshift, S3, and Lambda
- Building automated ETL workflows and optimizing data warehouse performance
- Implementing CI/CD pipelines for data engineering workflows

### 💼 Data Engineer @ Humana
**Jul 2023 - Jun 2024**
- Developing GCP-based healthcare data pipelines with BigQuery and Dataflow
- Ensuring HIPAA compliance and data governance for sensitive patient data
- Creating real-time analytics solutions for population health monitoring

### 💼 Data Engineer @ Unilever
**Jun 2020 - Dec 2022**
- Building Azure data engineering solutions with ADF, Databricks, and Synapse
- Designing data models for global sales and supply chain analytics
- Migrating legacy workloads to Azure cloud platform

---

## 🎓 Education & Certifications

**Master's in Industrial Technology**  
University of Central Missouri

**Bachelor's in Computer Science and Engineering**  
G Pullaiah College of Engineering and Technology

**Certifications**
- Microsoft Certified: Azure Administrator Associate (AZ-104)

---

## 📊 GitHub Stats

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=atmakur3&show_icons=true&theme=radical)

---

## 🤝 Let's Connect!

I'm passionate about leveraging cloud data technologies, machine learning integrations, and automation frameworks to continuously improve enterprise data ecosystems and deliver measurable business impact.

📧 **Email**: vanib.0398@gmail.com  
📱 **Phone**: +1-917-271-8933  
💼 **LinkedIn**: [Connect with me](https://www.linkedin.com/in/vani-atmakur)  
🔗 **GitHub**: [github.com/atmakur3](https://github.com/atmakur3)

---

*⭐ Feel free to explore my repositories and reach out for collaboration opportunities!*
